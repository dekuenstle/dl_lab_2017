{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Used a Python3 kernel to run this.\n",
    "#       It may work for Python2 kernels as well, \n",
    "#       but no garanties given.\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import time\n",
    "try:\n",
    "    import cPickle as pickle  # python2\n",
    "except ImportError:\n",
    "    import pickle # python3\n",
    "    \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import rc \n",
    "# use font and figure size for latex\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})\n",
    "rc('text', usetex=True)\n",
    "textwidth_tex_a4 = 6.27\n",
    "fig_factor = 0.9\n",
    "golden_ratio = 1.6180339887\n",
    "rc('figure', figsize=(fig_factor*textwidth_tex_a4, fig_factor*textwidth_tex_a4/golden_ratio))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing LeNet in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist(datasets_dir='./data'):\n",
    "    if not os.path.exists(datasets_dir):\n",
    "        os.mkdir(datasets_dir)\n",
    "    data_file = os.path.join(datasets_dir, 'mnist.pkl.gz')\n",
    "    if not os.path.exists(data_file):\n",
    "        print('... downloading MNIST from the web')\n",
    "        try:\n",
    "            import urllib\n",
    "            urllib.urlretrieve('http://google.com')\n",
    "        except AttributeError:\n",
    "            import urllib.request as urllib\n",
    "        url = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "        urllib.urlretrieve(url, data_file)\n",
    "\n",
    "    print('... loading data')\n",
    "    # Load the dataset\n",
    "    f = gzip.open(data_file, 'rb')\n",
    "    try:\n",
    "        train_set, valid_set, test_set = pickle.load(f, encoding=\"latin1\")\n",
    "    except TypeError:\n",
    "        train_set, valid_set, test_set = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    test_x, test_y = test_set\n",
    "    test_x = test_x.astype('float32')\n",
    "    test_x = test_x.astype('float32').reshape(test_x.shape[0], 1, 28, 28)\n",
    "    test_y = test_y.astype('int32')\n",
    "    valid_x, valid_y = valid_set\n",
    "    valid_x = valid_x.astype('float32')\n",
    "    valid_x = valid_x.astype('float32').reshape(valid_x.shape[0], 1, 28, 28)\n",
    "    valid_y = valid_y.astype('int32')\n",
    "    train_x, train_y = train_set\n",
    "    train_x = train_x.astype('float32').reshape(train_x.shape[0], 1, 28, 28)\n",
    "    train_y = train_y.astype('int32')\n",
    "    rval = [(train_x, train_y), (valid_x, valid_y), (test_x, test_y)]\n",
    "    print('... done loading data')\n",
    "    return rval\n",
    "\n",
    "def one_hot(labels):\n",
    "    \"\"\"this creates a one hot encoding from a flat vector:\n",
    "    i.e. given y = [0,2,1]\n",
    "     it creates y_one_hot = [[1,0,0], [0,0,1], [0,1,0]]\n",
    "    \"\"\"\n",
    "    classes = np.unique(labels)\n",
    "    n_classes = classes.size\n",
    "    one_hot_labels = np.zeros(labels.shape + (n_classes,))\n",
    "    for c in classes:\n",
    "        one_hot_labels[labels == c, c] = 1\n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "... done loading data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load\n",
    "Dtrain, Dval, Dtest = mnist()\n",
    "X_train, y_train = Dtrain\n",
    "X_valid, y_valid = Dval\n",
    "X_test, y_test = Dtest\n",
    "\n",
    "# Downsample training data to make it a bit faster for testing this code\n",
    "n_train_samples = 10000\n",
    "train_idxs = np.random.permutation(X_train.shape[0])[:n_train_samples]\n",
    "X_train_subset = X_train[train_idxs]\n",
    "y_train_subset = y_train[train_idxs]\n",
    "\n",
    "X_train = X_train.swapaxes(1,3)\n",
    "X_train_subset = X_train_subset.swapaxes(1,3)\n",
    "X_valid = X_valid.swapaxes(1,3)\n",
    "X_test = X_test.swapaxes(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet():\n",
    "    def __init__(self, init_stddev=0.1, n_filter=16, max_batch_size=10000):\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.X_placeholder = tf.placeholder(tf.float32, (None, 28, 28, 1))\n",
    "        self.y_placeholder = tf.placeholder(tf.float32, (None, 10))\n",
    "        self.network_graph = self._network_graph(init_stddev,\n",
    "                                                 n_filter)\n",
    "        self.classification_error_graph = self._classification_error_graph()\n",
    "        self.loss_graph = self._loss_graph()\n",
    "\n",
    "        \n",
    "    def _run(self, graph, X, y):\n",
    "        data = {self.X_placeholder: X, \n",
    "                self.y_placeholder: y}\n",
    "        return self._session.run(graph, feed_dict=data)\n",
    "    \n",
    "    def _run_batched(self, graph, X, y, batch_size):\n",
    "        n_samples = X.shape[0]\n",
    "        n_batches = int(np.ceil(n_samples / batch_size))\n",
    "        sizes = []\n",
    "        results = []\n",
    "        for b in range(n_batches):\n",
    "            batch_start = b * batch_size\n",
    "            batch_end = min(n_samples, batch_start + batch_size)\n",
    "        \n",
    "            X_batch = X[batch_start:batch_end,]\n",
    "            y_batch = y[batch_start:batch_end,]\n",
    "            batch_result = self._run(graph, X_batch, y_batch)\n",
    "            \n",
    "            sizes.append(batch_end - batch_start)\n",
    "            results.append(batch_result)\n",
    "        return sizes, results\n",
    "      \n",
    "    def _gd_step_graph(self, learning_rate):\n",
    "        gd = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        return gd.minimize(self._loss_graph())\n",
    "    \n",
    "    def _loss_graph(self):\n",
    "        negsum = -tf.reduce_sum(self.y_placeholder *\n",
    "                                    tf.log(self.network_graph), \n",
    "                                axis=[1])\n",
    "        cross_entropy = tf.reduce_mean(negsum)\n",
    "        return cross_entropy\n",
    "    \n",
    "    def _classification_error_graph(self):\n",
    "        wrong_pred = tf.not_equal(tf.argmax(self.y_placeholder, 1),\n",
    "                                  tf.argmax(self.network_graph, 1))\n",
    "        error_graph = tf.reduce_mean(tf.cast(wrong_pred, tf.float32))\n",
    "        return error_graph\n",
    "        \n",
    "    def loss(self, X, y):\n",
    "        batch_size, batch_loss = self._run_batched(self.loss_graph, \n",
    "                                                   X, y,\n",
    "                                                   self.max_batch_size)\n",
    "        n_samples = sum(batch_size)\n",
    "        return np.mean(np.array(batch_size)  / n_samples * np.array(batch_loss))\n",
    "        \n",
    "    def classification_error(self, X, y, y_one_hot=True):\n",
    "        if y_one_hot:\n",
    "            y = one_hot(y)\n",
    "        batch_size, batch_error = self._run_batched(self.classification_error_graph, \n",
    "                                                    X, y,\n",
    "                                                    self.max_batch_size)\n",
    "        n_samples = sum(batch_size)\n",
    "        return np.mean(np.array(batch_size) / n_samples * np.array(batch_error))\n",
    "        \n",
    "\n",
    "    def _network_graph(self, init_stddev, n_filter):\n",
    "        def conv(x):\n",
    "            filter_shape = [3, 3, x.shape[-1].value, n_filter]\n",
    "            W = tf.Variable(tf.random_normal(filter_shape, \n",
    "                                             stddev=init_stddev))\n",
    "            b = tf.Variable(tf.random_normal(filter_shape[-1:],\n",
    "                                             stddev=init_stddev))\n",
    "            return tf.nn.conv2d(x, W,             \n",
    "                                strides=[1, 1, 1, 1],\n",
    "                                padding='SAME') + b\n",
    "        def max_pool(x):\n",
    "            return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                                  strides=[1, 2, 2, 1],\n",
    "                                  padding='SAME')\n",
    "\n",
    "        def fully_connected(x, n_units):\n",
    "            x_flat = tf.reshape(x, [-1, np.prod(x.shape[1:]).value])\n",
    "            W = tf.Variable(tf.random_normal([x_flat.shape[1].value, n_units], \n",
    "                                             stddev=init_stddev))\n",
    "            b = tf.Variable(tf.random_normal([n_units], \n",
    "                                             stddev=init_stddev))\n",
    "            return tf.matmul(x_flat,W) + b\n",
    "    \n",
    "        x = self.X_placeholder\n",
    "        conv1 = conv(x)\n",
    "        act1 = tf.nn.relu(conv1)\n",
    "        pool1 = max_pool(act1)\n",
    "        conv2 = conv(pool1)\n",
    "        act2 = tf.nn.relu(conv2)\n",
    "        pool2 = max_pool(act2)\n",
    "        full1 = fully_connected(pool2, n_units=128)\n",
    "        full2 = fully_connected(full1, n_units=10)\n",
    "        y_pred = tf.nn.softmax(full2)\n",
    "        return y_pred\n",
    "    \n",
    "    def sdg_epoch(self, X, y, learning_rate, batch_size):\n",
    "        step = self._gd_step_graph(learning_rate)\n",
    "        return self._run_batched(step, X, y, batch_size)\n",
    "     \n",
    "    def train(self, X, y, X_valid, y_valid, learning_rate=0.1,\n",
    "              max_epochs=100, batch_size=64, y_one_hot=True):\n",
    "        print(\"... setup training\")\n",
    "        if y_one_hot:\n",
    "            y_1hot = one_hot(y)\n",
    "            y_valid_1hot = one_hot(y_valid)\n",
    "        else:\n",
    "            y_1hot = y\n",
    "            y_valid_1hot = y_valid\n",
    "        trace = dict(train_loss=[], train_error=[], valid_error=[])\n",
    "       \n",
    "        print(\"... starting training\")\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            self._session = sess\n",
    "            for e in range(1, max_epochs+1):\n",
    "                self.sdg_epoch(X, y_1hot, learning_rate, batch_size)\n",
    "                \n",
    "                train_loss = self.loss(X, y_1hot)\n",
    "                train_error = self.classification_error(X, y_1hot,\n",
    "                                                        y_one_hot=False)\n",
    "                valid_error = self.classification_error(X_valid, y_valid_1hot,\n",
    "                                                        y_one_hot=False)\n",
    "                trace['train_loss'].append(train_loss)\n",
    "                trace['train_error'].append(train_error)\n",
    "                trace['valid_error'].append(valid_error) \n",
    "                print(('epoch {}: loss {:.4f}, ' +\n",
    "                        'train error {:.4f}, ' + \n",
    "                        'valid error {:.4f}' +\n",
    "                        '').format(e, train_loss, \n",
    "                                   train_error, valid_error))\n",
    "        return trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the Learning Rate\n",
    "\n",
    "Which conclusions could be drawn from this figure?\n",
    "\n",
    "Which value for the learning rate works best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE LEARN RATE 0.1\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0249, train error 0.0077, valid error 0.0335\n",
      "epoch 2: loss 0.0158, train error 0.0050, valid error 0.0243\n",
      "epoch 3: loss 0.0121, train error 0.0038, valid error 0.0201\n",
      "epoch 4: loss 0.0101, train error 0.0032, valid error 0.0188\n",
      "epoch 5: loss 0.0087, train error 0.0027, valid error 0.0183\n",
      "epoch 6: loss 0.0077, train error 0.0024, valid error 0.0175\n",
      "epoch 7: loss 0.0070, train error 0.0021, valid error 0.0163\n",
      "epoch 8: loss 0.0065, train error 0.0020, valid error 0.0163\n",
      "epoch 9: loss 0.0061, train error 0.0020, valid error 0.0153\n",
      "epoch 10: loss 0.0056, train error 0.0019, valid error 0.0154\n",
      "epoch 11: loss 0.0053, train error 0.0017, valid error 0.0152\n",
      "epoch 12: loss 0.0051, train error 0.0016, valid error 0.0150\n",
      "epoch 13: loss 0.0048, train error 0.0016, valid error 0.0151\n",
      "epoch 14: loss 0.0047, train error 0.0016, valid error 0.0152\n",
      "epoch 15: loss 0.0045, train error 0.0015, valid error 0.0150\n",
      "USE LEARN RATE 0.01\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0721, train error 0.0214, valid error 0.0912\n",
      "epoch 2: loss 0.0472, train error 0.0141, valid error 0.0606\n",
      "epoch 3: loss 0.0356, train error 0.0107, valid error 0.0467\n",
      "epoch 4: loss 0.0286, train error 0.0087, valid error 0.0378\n",
      "epoch 5: loss 0.0243, train error 0.0074, valid error 0.0314\n",
      "epoch 6: loss 0.0214, train error 0.0067, valid error 0.0278\n",
      "epoch 7: loss 0.0195, train error 0.0060, valid error 0.0261\n",
      "epoch 8: loss 0.0180, train error 0.0055, valid error 0.0251\n",
      "epoch 9: loss 0.0169, train error 0.0052, valid error 0.0241\n",
      "epoch 10: loss 0.0159, train error 0.0049, valid error 0.0233\n",
      "epoch 11: loss 0.0151, train error 0.0047, valid error 0.0229\n",
      "epoch 12: loss 0.0144, train error 0.0044, valid error 0.0221\n",
      "epoch 13: loss 0.0138, train error 0.0043, valid error 0.0211\n",
      "epoch 14: loss 0.0133, train error 0.0041, valid error 0.0208\n",
      "epoch 15: loss 0.0128, train error 0.0039, valid error 0.0202\n",
      "USE LEARN RATE 0.001\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.4286, train error 0.1377, valid error 0.6709\n",
      "epoch 2: loss 0.2875, train error 0.0694, valid error 0.3207\n",
      "epoch 3: loss 0.1329, train error 0.0374, valid error 0.1615\n",
      "epoch 4: loss 0.0957, train error 0.0283, valid error 0.1218\n",
      "epoch 5: loss 0.0809, train error 0.0242, valid error 0.1059\n",
      "epoch 6: loss 0.0724, train error 0.0216, valid error 0.0953\n",
      "epoch 7: loss 0.0664, train error 0.0199, valid error 0.0886\n",
      "epoch 8: loss 0.0620, train error 0.0185, valid error 0.0819\n",
      "epoch 9: loss 0.0583, train error 0.0174, valid error 0.0766\n",
      "epoch 10: loss 0.0553, train error 0.0164, valid error 0.0720\n",
      "epoch 11: loss 0.0527, train error 0.0157, valid error 0.0690\n",
      "epoch 12: loss 0.0505, train error 0.0150, valid error 0.0672\n",
      "epoch 13: loss 0.0485, train error 0.0145, valid error 0.0641\n",
      "epoch 14: loss 0.0467, train error 0.0139, valid error 0.0624\n",
      "epoch 15: loss 0.0451, train error 0.0134, valid error 0.0606\n",
      "USE LEARN RATE 0.0001\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.4623, train error 0.1788, valid error 0.8894\n",
      "epoch 2: loss 0.4557, train error 0.1694, valid error 0.8436\n",
      "epoch 3: loss 0.4509, train error 0.1603, valid error 0.7905\n",
      "epoch 4: loss 0.4464, train error 0.1536, valid error 0.7581\n",
      "epoch 5: loss 0.4416, train error 0.1482, valid error 0.7311\n",
      "epoch 6: loss 0.4363, train error 0.1435, valid error 0.7102\n",
      "epoch 7: loss 0.4302, train error 0.1386, valid error 0.6858\n",
      "epoch 8: loss 0.4232, train error 0.1334, valid error 0.6607\n",
      "epoch 9: loss 0.4151, train error 0.1276, valid error 0.6325\n",
      "epoch 10: loss 0.4055, train error 0.1208, valid error 0.5965\n",
      "epoch 11: loss 0.3940, train error 0.1132, valid error 0.5588\n",
      "epoch 12: loss 0.3804, train error 0.1049, valid error 0.5170\n",
      "epoch 13: loss 0.3643, train error 0.0961, valid error 0.4666\n",
      "epoch 14: loss 0.3455, train error 0.0881, valid error 0.4218\n",
      "epoch 15: loss 0.3242, train error 0.0805, valid error 0.3800\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "error_per_learning_rate = dict()\n",
    "\n",
    "train_config = dict(max_epochs=15, batch_size=64,\n",
    "                    y_one_hot=True)\n",
    "\n",
    "lenet = LeNet(init_stddev=0.1, n_filter=16)\n",
    "for learning_rate in learning_rates:\n",
    "    print(\"USE LEARN RATE {}\".format(learning_rate))\n",
    "    trace = lenet.train(X_train, y_train,\n",
    "                        X_valid, y_valid,\n",
    "                        learning_rate=learning_rate,\n",
    "                        **train_config)\n",
    "    error_per_learning_rate[str(learning_rate)] = trace['valid_error']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAADwCAYAAAAkTF41AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5c5d119390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for learning_rate, error in error_per_learning_rate.items():\n",
    "    plt.plot(error, label=learning_rate)\n",
    "plt.legend(title='learning rate', loc='best')\n",
    "plt.ylim((0, 1))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error rate')\n",
    "\n",
    "plt.savefig('../report/assets/error.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime CPU vs GPU\n",
    "\n",
    "How does the number of parameters\n",
    "change if you increase the number of filters? \n",
    "\n",
    "How does the runtime change? \n",
    "\n",
    "At last, run the same experiments again on a CPU, How does the plot change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_filters = [8, 16, 32, 64, 128, 256]\n",
    "cpu_filters = [8, 16, 32, 64]\n",
    "\n",
    "devices = device_lib.list_local_devices()\n",
    "cpus = [d for d in devices if d.device_type == 'CPU']\n",
    "gpus = [d for d in devices if d.device_type == 'GPU']\n",
    "\n",
    "train_config = dict(max_epochs=10, batch_size=64,\n",
    "                    learning_rate=0.1, y_one_hot=True)\n",
    "\n",
    "def check_time(filters):\n",
    "    times = []\n",
    "    for n_filter in filters:\n",
    "        print(\"USE #FILTER {}\".format(n_filter))\n",
    "        start = time.time()\n",
    "        lenet = LeNet(init_stddev=0.1, \n",
    "                      n_filter=n_filter, \n",
    "                      max_batch_size=int(32/n_filter*10000))\n",
    "        trace = lenet.train(X_train, y_train,\n",
    "                            X_valid, y_valid,\n",
    "                            **train_config)\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE GPU\n",
      "USE #FILTER 8\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0840, train error 0.0257, valid error 0.0467\n",
      "epoch 2: loss 0.0562, train error 0.0172, valid error 0.0305\n",
      "epoch 3: loss 0.0454, train error 0.0139, valid error 0.0255\n",
      "epoch 4: loss 0.0389, train error 0.0118, valid error 0.0228\n",
      "epoch 5: loss 0.0339, train error 0.0105, valid error 0.0207\n",
      "epoch 6: loss 0.0314, train error 0.0099, valid error 0.0202\n",
      "epoch 7: loss 0.0293, train error 0.0094, valid error 0.0192\n",
      "epoch 8: loss 0.0283, train error 0.0093, valid error 0.0180\n",
      "epoch 9: loss 0.0267, train error 0.0087, valid error 0.0172\n",
      "epoch 10: loss 0.0257, train error 0.0083, valid error 0.0168\n",
      "USE #FILTER 16\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0361, train error 0.0114, valid error 0.0302\n",
      "epoch 2: loss 0.0252, train error 0.0081, valid error 0.0233\n",
      "epoch 3: loss 0.0196, train error 0.0062, valid error 0.0194\n",
      "epoch 4: loss 0.0173, train error 0.0056, valid error 0.0181\n",
      "epoch 5: loss 0.0156, train error 0.0051, valid error 0.0168\n",
      "epoch 6: loss 0.0145, train error 0.0048, valid error 0.0164\n",
      "epoch 7: loss 0.0131, train error 0.0044, valid error 0.0166\n",
      "epoch 8: loss 0.0118, train error 0.0040, valid error 0.0158\n",
      "epoch 9: loss 0.0109, train error 0.0037, valid error 0.0159\n",
      "epoch 10: loss 0.0099, train error 0.0034, valid error 0.0154\n",
      "USE #FILTER 32\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0195, train error 0.0061, valid error 0.0259\n",
      "epoch 2: loss 0.0134, train error 0.0043, valid error 0.0196\n",
      "epoch 3: loss 0.0125, train error 0.0040, valid error 0.0212\n",
      "epoch 4: loss 0.0103, train error 0.0033, valid error 0.0196\n",
      "epoch 5: loss 0.0076, train error 0.0025, valid error 0.0168\n",
      "epoch 6: loss 0.0065, train error 0.0022, valid error 0.0148\n",
      "epoch 7: loss 0.0057, train error 0.0019, valid error 0.0139\n",
      "epoch 8: loss 0.0049, train error 0.0017, valid error 0.0137\n",
      "epoch 9: loss 0.0045, train error 0.0015, valid error 0.0139\n",
      "epoch 10: loss 0.0043, train error 0.0015, valid error 0.0146\n",
      "USE #FILTER 64\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0093, train error 0.0029, valid error 0.0123\n",
      "epoch 2: loss 0.0057, train error 0.0017, valid error 0.0096\n",
      "epoch 3: loss 0.0043, train error 0.0013, valid error 0.0080\n",
      "epoch 4: loss 0.0035, train error 0.0011, valid error 0.0075\n",
      "epoch 5: loss 0.0031, train error 0.0010, valid error 0.0077\n",
      "epoch 6: loss 0.0027, train error 0.0009, valid error 0.0075\n",
      "epoch 7: loss 0.0023, train error 0.0008, valid error 0.0073\n",
      "epoch 8: loss 0.0021, train error 0.0007, valid error 0.0073\n",
      "epoch 9: loss 0.0018, train error 0.0006, valid error 0.0073\n",
      "epoch 10: loss 0.0016, train error 0.0006, valid error 0.0072\n",
      "USE #FILTER 128\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0050, train error 0.0015, valid error 0.0065\n",
      "epoch 2: loss 0.0030, train error 0.0009, valid error 0.0048\n",
      "epoch 3: loss 0.0024, train error 0.0007, valid error 0.0042\n",
      "epoch 4: loss 0.0019, train error 0.0006, valid error 0.0037\n",
      "epoch 5: loss 0.0016, train error 0.0005, valid error 0.0037\n",
      "epoch 6: loss 0.0014, train error 0.0004, valid error 0.0037\n",
      "epoch 7: loss 0.0012, train error 0.0004, valid error 0.0037\n",
      "epoch 8: loss 0.0011, train error 0.0004, valid error 0.0038\n",
      "epoch 9: loss 0.0011, train error 0.0004, valid error 0.0037\n",
      "epoch 10: loss 0.0011, train error 0.0004, valid error 0.0036\n",
      "USE #FILTER 256\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 2: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 3: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 4: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 5: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 6: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 7: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 8: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 9: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 10: loss nan, train error 0.0225, valid error 0.1126\n",
      "USE CPU\n",
      "USE #FILTER 8\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0713, train error 0.0219, valid error 0.0381\n",
      "epoch 2: loss 0.0503, train error 0.0158, valid error 0.0258\n",
      "epoch 3: loss 0.0419, train error 0.0131, valid error 0.0239\n",
      "epoch 4: loss 0.0384, train error 0.0119, valid error 0.0230\n",
      "epoch 5: loss 0.0352, train error 0.0109, valid error 0.0223\n",
      "epoch 6: loss 0.0327, train error 0.0102, valid error 0.0214\n",
      "epoch 7: loss 0.0304, train error 0.0095, valid error 0.0201\n",
      "epoch 8: loss 0.0287, train error 0.0089, valid error 0.0197\n",
      "epoch 9: loss 0.0273, train error 0.0084, valid error 0.0189\n",
      "epoch 10: loss 0.0263, train error 0.0082, valid error 0.0189\n",
      "USE #FILTER 16\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0346, train error 0.0107, valid error 0.0301\n",
      "epoch 2: loss 0.0239, train error 0.0077, valid error 0.0223\n",
      "epoch 3: loss 0.0195, train error 0.0062, valid error 0.0201\n",
      "epoch 4: loss 0.0166, train error 0.0054, valid error 0.0190\n",
      "epoch 5: loss 0.0154, train error 0.0050, valid error 0.0184\n",
      "epoch 6: loss 0.0138, train error 0.0046, valid error 0.0179\n",
      "epoch 7: loss 0.0131, train error 0.0044, valid error 0.0173\n",
      "epoch 8: loss 0.0125, train error 0.0042, valid error 0.0170\n",
      "epoch 9: loss 0.0119, train error 0.0040, valid error 0.0169\n",
      "epoch 10: loss 0.0117, train error 0.0041, valid error 0.0171\n",
      "USE #FILTER 32\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0226, train error 0.0072, valid error 0.0308\n",
      "epoch 2: loss 0.0143, train error 0.0046, valid error 0.0223\n",
      "epoch 3: loss 0.0110, train error 0.0034, valid error 0.0185\n",
      "epoch 4: loss 0.0091, train error 0.0028, valid error 0.0171\n",
      "epoch 5: loss 0.0079, train error 0.0025, valid error 0.0157\n",
      "epoch 6: loss 0.0068, train error 0.0022, valid error 0.0147\n",
      "epoch 7: loss 0.0060, train error 0.0019, valid error 0.0146\n",
      "epoch 8: loss 0.0054, train error 0.0017, valid error 0.0148\n",
      "epoch 9: loss 0.0050, train error 0.0016, valid error 0.0144\n",
      "epoch 10: loss 0.0046, train error 0.0015, valid error 0.0137\n",
      "USE #FILTER 64\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0089, train error 0.0028, valid error 0.0120\n",
      "epoch 2: loss 0.0058, train error 0.0018, valid error 0.0092\n",
      "epoch 3: loss 0.0043, train error 0.0013, valid error 0.0075\n",
      "epoch 4: loss 0.0036, train error 0.0011, valid error 0.0067\n",
      "epoch 5: loss 0.0032, train error 0.0010, valid error 0.0066\n",
      "epoch 6: loss 0.0029, train error 0.0009, valid error 0.0064\n",
      "epoch 7: loss 0.0025, train error 0.0008, valid error 0.0063\n",
      "epoch 8: loss 0.0023, train error 0.0008, valid error 0.0062\n",
      "epoch 9: loss 0.0020, train error 0.0007, valid error 0.0062\n",
      "epoch 10: loss 0.0018, train error 0.0006, valid error 0.0063\n"
     ]
    }
   ],
   "source": [
    "gpu_time = []\n",
    "cpu_time = []\n",
    "\n",
    "if len(gpus) > 0:\n",
    "    print(\"USE GPU\")\n",
    "    with tf.device(gpus[0].name):\n",
    "        gpu_time = check_time(gpu_filters)\n",
    "        \n",
    "if len(cpus) > 0:\n",
    "    print(\"USE CPU\")\n",
    "    with tf.device(cpus[0].name):\n",
    "        cpu_time = check_time(cpu_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAADsCAYAAAC7byfWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5c25b02ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if len(cpu_filters) == len(cpu_time):\n",
    "    plt.scatter(cpu_filters, cpu_time, label='CPU', c='b')\n",
    "\n",
    "if len(gpu_filters) == len(gpu_time):\n",
    "    plt.scatter(gpu_filters, gpu_time, label='GPU', c='g')\n",
    "    \n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('filter [\\#]')\n",
    "plt.ylim(-50, 950)\n",
    "plt.xscale('log', basex=2)\n",
    "plt.ylabel('runtime [s]')\n",
    "\n",
    "plt.savefig('../report/assets/runtime.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
