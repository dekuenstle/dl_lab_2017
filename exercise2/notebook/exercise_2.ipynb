{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Used a Python3 kernel to run this.\n",
    "#       It may work for Python2 kernels as well, \n",
    "#       but no garanties given.\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import time\n",
    "try:\n",
    "    import cPickle as pickle  # python2\n",
    "except ImportError:\n",
    "    import pickle # python3\n",
    "    \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "%matplotlib inline\n",
    "from matplotlib import rc \n",
    "# use font and figure size for latex\n",
    "rc('font', **{'family': 'serif', 'serif': ['Computer Modern']})\n",
    "rc('text', usetex=True)\n",
    "textwidth_tex_a4 = 6.27\n",
    "fig_factor = 0.9\n",
    "golden_ratio = 1.6180339887\n",
    "rc('figure', figsize=(fig_factor*textwidth_tex_a4, fig_factor*textwidth_tex_a4/golden_ratio))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing LeNet in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist(datasets_dir='./data'):\n",
    "    if not os.path.exists(datasets_dir):\n",
    "        os.mkdir(datasets_dir)\n",
    "    data_file = os.path.join(datasets_dir, 'mnist.pkl.gz')\n",
    "    if not os.path.exists(data_file):\n",
    "        print('... downloading MNIST from the web')\n",
    "        try:\n",
    "            import urllib\n",
    "            urllib.urlretrieve('http://google.com')\n",
    "        except AttributeError:\n",
    "            import urllib.request as urllib\n",
    "        url = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "        urllib.urlretrieve(url, data_file)\n",
    "\n",
    "    print('... loading data')\n",
    "    # Load the dataset\n",
    "    f = gzip.open(data_file, 'rb')\n",
    "    try:\n",
    "        train_set, valid_set, test_set = pickle.load(f, encoding=\"latin1\")\n",
    "    except TypeError:\n",
    "        train_set, valid_set, test_set = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    test_x, test_y = test_set\n",
    "    test_x = test_x.astype('float32')\n",
    "    test_x = test_x.astype('float32').reshape(test_x.shape[0], 1, 28, 28)\n",
    "    test_y = test_y.astype('int32')\n",
    "    valid_x, valid_y = valid_set\n",
    "    valid_x = valid_x.astype('float32')\n",
    "    valid_x = valid_x.astype('float32').reshape(valid_x.shape[0], 1, 28, 28)\n",
    "    valid_y = valid_y.astype('int32')\n",
    "    train_x, train_y = train_set\n",
    "    train_x = train_x.astype('float32').reshape(train_x.shape[0], 1, 28, 28)\n",
    "    train_y = train_y.astype('int32')\n",
    "    rval = [(train_x, train_y), (valid_x, valid_y), (test_x, test_y)]\n",
    "    print('... done loading data')\n",
    "    return rval\n",
    "\n",
    "def one_hot(labels):\n",
    "    \"\"\"this creates a one hot encoding from a flat vector:\n",
    "    i.e. given y = [0,2,1]\n",
    "     it creates y_one_hot = [[1,0,0], [0,0,1], [0,1,0]]\n",
    "    \"\"\"\n",
    "    classes = np.unique(labels)\n",
    "    n_classes = classes.size\n",
    "    one_hot_labels = np.zeros(labels.shape + (n_classes,))\n",
    "    for c in classes:\n",
    "        one_hot_labels[labels == c, c] = 1\n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "... done loading data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load\n",
    "Dtrain, Dval, Dtest = mnist()\n",
    "X_train, y_train = Dtrain\n",
    "X_valid, y_valid = Dval\n",
    "X_test, y_test = Dtest\n",
    "\n",
    "# Downsample training data to make it a bit faster for testing this code\n",
    "n_train_samples = 10000\n",
    "train_idxs = np.random.permutation(X_train.shape[0])[:n_train_samples]\n",
    "X_train_subset = X_train[train_idxs]\n",
    "y_train_subset = y_train[train_idxs]\n",
    "\n",
    "X_train = X_train.swapaxes(1,3)\n",
    "X_train_subset = X_train_subset.swapaxes(1,3)\n",
    "X_valid = X_valid.swapaxes(1,3)\n",
    "X_test = X_test.swapaxes(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet():\n",
    "    def __init__(self, init_stddev=0.1, n_filter=16, max_batch_size=10000):\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.X_placeholder = tf.placeholder(tf.float32, (None, 28, 28, 1))\n",
    "        self.y_placeholder = tf.placeholder(tf.float32, (None, 10))\n",
    "        self.network_graph = self._network_graph(init_stddev,\n",
    "                                                 n_filter)\n",
    "        self.classification_error_graph = self._classification_error_graph()\n",
    "        self.loss_graph = self._loss_graph()\n",
    "\n",
    "        \n",
    "    def _run(self, graph, X, y):\n",
    "        data = {self.X_placeholder: X, \n",
    "                self.y_placeholder: y}\n",
    "        return self._session.run(graph, feed_dict=data)\n",
    "    \n",
    "    def _run_batched(self, graph, X, y, batch_size):\n",
    "        n_samples = X.shape[0]\n",
    "        n_batches = int(np.ceil(n_samples / batch_size))\n",
    "        sizes = []\n",
    "        results = []\n",
    "        for b in range(n_batches):\n",
    "            batch_start = b * batch_size\n",
    "            batch_end = min(n_samples, batch_start + batch_size)\n",
    "        \n",
    "            X_batch = X[batch_start:batch_end,]\n",
    "            y_batch = y[batch_start:batch_end,]\n",
    "            batch_result = self._run(graph, X_batch, y_batch)\n",
    "            \n",
    "            sizes.append(batch_end - batch_start)\n",
    "            results.append(batch_result)\n",
    "        return sizes, results\n",
    "      \n",
    "    def _gd_step_graph(self, learning_rate):\n",
    "        gd = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        return gd.minimize(self._loss_graph())\n",
    "    \n",
    "    def _loss_graph(self):\n",
    "        negsum = -tf.reduce_sum(self.y_placeholder *\n",
    "                                    tf.log(self.network_graph), \n",
    "                                axis=[1])\n",
    "        cross_entropy = tf.reduce_mean(negsum)\n",
    "        return cross_entropy\n",
    "    \n",
    "    def _classification_error_graph(self):\n",
    "        wrong_pred = tf.not_equal(tf.argmax(self.y_placeholder, 1),\n",
    "                                  tf.argmax(self.network_graph, 1))\n",
    "        error_graph = tf.reduce_mean(tf.cast(wrong_pred, tf.float32))\n",
    "        return error_graph\n",
    "        \n",
    "    def loss(self, X, y):\n",
    "        batch_size, batch_loss = self._run_batched(self.loss_graph, \n",
    "                                                   X, y,\n",
    "                                                   self.max_batch_size)\n",
    "        n_samples = sum(batch_size)\n",
    "        return np.mean(np.array(batch_size) * np.array(batch_loss) / n_samples)\n",
    "        \n",
    "    def classification_error(self, X, y, y_one_hot=True):\n",
    "        if y_one_hot:\n",
    "            y = one_hot(y)\n",
    "        batch_size, batch_error = self._run_batched(self.classification_error_graph, \n",
    "                                                    X, y,\n",
    "                                                    self.max_batch_size)\n",
    "        n_samples = sum(batch_size)\n",
    "        return np.mean(np.array(batch_size) * np.array(batch_error) / n_samples)\n",
    "        \n",
    "\n",
    "    def _network_graph(self, init_stddev, n_filter):\n",
    "        def conv(x):\n",
    "            filter_shape = [3, 3, x.shape[-1].value, n_filter]\n",
    "            W = tf.Variable(tf.random_normal(filter_shape, \n",
    "                                             stddev=init_stddev))\n",
    "            b = tf.Variable(tf.random_normal(filter_shape[-1:],\n",
    "                                             stddev=init_stddev))\n",
    "            return tf.nn.conv2d(x, W,             \n",
    "                                strides=[1, 1, 1, 1],\n",
    "                                padding='SAME') + b\n",
    "        def max_pool(x):\n",
    "            return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                                  strides=[1, 2, 2, 1],\n",
    "                                  padding='SAME')\n",
    "\n",
    "        def fully_connected(x, n_units):\n",
    "            x_flat = tf.reshape(x, [-1, np.prod(x.shape[1:]).value])\n",
    "            W = tf.Variable(tf.random_normal([x_flat.shape[1].value, n_units], \n",
    "                                             stddev=init_stddev))\n",
    "            b = tf.Variable(tf.random_normal([n_units], \n",
    "                                             stddev=init_stddev))\n",
    "            return tf.matmul(x_flat,W) + b\n",
    "    \n",
    "        x = self.X_placeholder\n",
    "        conv1 = conv(x)\n",
    "        act1 = tf.nn.relu(conv1)\n",
    "        pool1 = max_pool(act1)\n",
    "        conv2 = conv(pool1)\n",
    "        act2 = tf.nn.relu(conv2)\n",
    "        pool2 = max_pool(act2)\n",
    "        full1 = fully_connected(pool2, n_units=128)\n",
    "        full2 = fully_connected(full1, n_units=10)\n",
    "        y_pred = tf.nn.softmax(full2)\n",
    "        return y_pred\n",
    "    \n",
    "    def sdg_epoch(self, X, y, learning_rate, batch_size):\n",
    "        step = self._gd_step_graph(learning_rate)\n",
    "        return self._run_batched(step, X, y, batch_size)\n",
    "     \n",
    "    def train(self, X, y, X_valid, y_valid, learning_rate=0.1,\n",
    "              max_epochs=100, batch_size=64, y_one_hot=True):\n",
    "        print(\"... setup training\")\n",
    "        if y_one_hot:\n",
    "            y_1hot = one_hot(y)\n",
    "            y_valid_1hot = one_hot(y_valid)\n",
    "        else:\n",
    "            y_1hot = y\n",
    "            y_valid_1hot = y_valid\n",
    "        trace = dict(train_loss=[], train_error=[], valid_error=[])\n",
    "       \n",
    "        print(\"... starting training\")\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            self._session = sess\n",
    "            for e in range(1, max_epochs+1):\n",
    "                self.sdg_epoch(X, y_1hot, learning_rate, batch_size)\n",
    "                \n",
    "                train_loss = self.loss(X, y_1hot)\n",
    "                train_error = self.classification_error(X, y_1hot,\n",
    "                                                        y_one_hot=False)\n",
    "                valid_error = self.classification_error(X_valid, y_valid_1hot,\n",
    "                                                        y_one_hot=False)\n",
    "                trace['train_loss'].append(train_loss)\n",
    "                trace['train_error'].append(train_error)\n",
    "                trace['valid_error'].append(valid_error) \n",
    "                print(('epoch {}: loss {:.4f}, ' +\n",
    "                        'train error {:.4f}, ' + \n",
    "                        'valid error {:.4f}' +\n",
    "                        '').format(e, train_loss, \n",
    "                                   train_error, valid_error))\n",
    "        return trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the Learning Rate\n",
    "\n",
    "Which conclusions could be drawn from this figure?\n",
    "\n",
    "Which value for the learning rate works best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE LEARN RATE 0.1\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0222, train error 0.0070, valid error 0.0284\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4677ccab48bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                         \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                         \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                         **train_config)\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0merror_per_learning_rate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'valid_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5b59af5612cd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y, X_valid, y_valid, learning_rate, max_epochs, batch_size, y_one_hot)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdg_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_1hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_1hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5b59af5612cd>\u001b[0m in \u001b[0;36msdg_epoch\u001b[0;34m(self, X, y, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msdg_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gd_step_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     def train(self, X, y, X_valid, y_valid, learning_rate=0.1,\n",
      "\u001b[0;32m<ipython-input-6-5b59af5612cd>\u001b[0m in \u001b[0;36m_run_batched\u001b[0;34m(self, graph, X, y, batch_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mbatch_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0msizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_end\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbatch_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5b59af5612cd>\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, graph, X, y)\u001b[0m\n\u001b[1;32m     13\u001b[0m         data = {self.X_placeholder: X, \n\u001b[1;32m     14\u001b[0m                 self.y_placeholder: y}\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
    "error_per_learning_rate = dict()\n",
    "\n",
    "train_config = dict(max_epochs=15, batch_size=64,\n",
    "                    y_one_hot=True)\n",
    "\n",
    "lenet = LeNet(init_stddev=0.1, n_filter=16)\n",
    "for learning_rate in learning_rates:\n",
    "    print(\"USE LEARN RATE {}\".format(learning_rate))\n",
    "    trace = lenet.train(X_train, y_train,\n",
    "                        X_valid, y_valid,\n",
    "                        learning_rate=learning_rate,\n",
    "                        **train_config)\n",
    "    error_per_learning_rate[str(learning_rate)] = trace['valid_error']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAADsCAYAAAC/mvfrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0f47184518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for (learning_rate, error), line_style in zip(error_per_learning_rate.items(),\n",
    "                                              ['-', '--', '-.', ':']):\n",
    "    plt.plot(error, label=learning_rate, linestyle=line_style)\n",
    "plt.legend(title='learning rate', loc='best')\n",
    "plt.ylim((0, 1))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('error rate')\n",
    "\n",
    "plt.savefig('../report/assets/error.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime CPU vs GPU\n",
    "\n",
    "How does the number of parameters\n",
    "change if you increase the number of filters? \n",
    "\n",
    "How does the runtime change? \n",
    "\n",
    "At last, run the same experiments again on a CPU, How does the plot change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_filters = [8, 16, 32, 64, 128, 256]\n",
    "cpu_filters = [8, 16, 32, 64]\n",
    "\n",
    "devices = device_lib.list_local_devices()\n",
    "cpus = [d for d in devices if d.device_type == 'CPU']\n",
    "gpus = [d for d in devices if d.device_type == 'GPU']\n",
    "\n",
    "train_config = dict(max_epochs=10, batch_size=64,\n",
    "                    learning_rate=0.1, y_one_hot=True)\n",
    "\n",
    "def check_time(filters):color\n",
    "    times = []\n",
    "    for n_filter in filters:\n",
    "        print(\"USE #FILTER {}\".format(n_filter))\n",
    "        start = time.time()\n",
    "        lenet = LeNet(init_stddev=0.1, \n",
    "                      n_filter=n_filter, \n",
    "                      max_batch_size=int(32/n_filter*10000))\n",
    "        trace = lenet.train(X_train, y_train,\n",
    "                            X_valid, y_valid,\n",
    "                            **train_config)\n",
    "        end = time.time()\n",
    "        times.append(end - start)\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE GPU\n",
      "USE #FILTER 8\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0777, train error 0.0249, valid error 0.0394\n",
      "epoch 2: loss 0.0652, train error 0.0212, valid error 0.0358\n",
      "epoch 3: loss 0.0478, train error 0.0159, valid error 0.0293\n",
      "epoch 4: loss 0.0405, train error 0.0133, valid error 0.0267\n",
      "epoch 5: loss 0.0364, train error 0.0118, valid error 0.0240\n",
      "epoch 6: loss 0.0333, train error 0.0109, valid error 0.0235\n",
      "epoch 7: loss 0.0322, train error 0.0106, valid error 0.0224\n",
      "epoch 8: loss 0.0301, train error 0.0099, valid error 0.0215\n",
      "epoch 9: loss 0.0287, train error 0.0094, valid error 0.0205\n",
      "epoch 10: loss 0.0276, train error 0.0090, valid error 0.0197\n",
      "USE #FILTER 16\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0399, train error 0.0125, valid error 0.0298\n",
      "epoch 2: loss 0.0269, train error 0.0085, valid error 0.0230\n",
      "epoch 3: loss 0.0212, train error 0.0066, valid error 0.0201\n",
      "epoch 4: loss 0.0179, train error 0.0055, valid error 0.0184\n",
      "epoch 5: loss 0.0162, train error 0.0052, valid error 0.0176\n",
      "epoch 6: loss 0.0150, train error 0.0049, valid error 0.0163\n",
      "epoch 7: loss 0.0137, train error 0.0045, valid error 0.0153\n",
      "epoch 8: loss 0.0126, train error 0.0042, valid error 0.0154\n",
      "epoch 9: loss 0.0113, train error 0.0038, valid error 0.0147\n",
      "epoch 10: loss 0.0099, train error 0.0034, valid error 0.0141\n",
      "USE #FILTER 32\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0172, train error 0.0054, valid error 0.0241\n",
      "epoch 2: loss 0.0123, train error 0.0040, valid error 0.0182\n",
      "epoch 3: loss 0.0097, train error 0.0032, valid error 0.0172\n",
      "epoch 4: loss 0.0082, train error 0.0026, valid error 0.0151\n",
      "epoch 5: loss 0.0068, train error 0.0022, valid error 0.0147\n",
      "epoch 6: loss 0.0061, train error 0.0019, valid error 0.0141\n",
      "epoch 7: loss 0.0054, train error 0.0018, valid error 0.0137\n",
      "epoch 8: loss 0.0049, train error 0.0016, valid error 0.0138\n",
      "epoch 9: loss 0.0046, train error 0.0016, valid error 0.0128\n",
      "epoch 10: loss 0.0041, train error 0.0014, valid error 0.0119\n",
      "USE #FILTER 64\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0085, train error 0.0027, valid error 0.0119\n",
      "epoch 2: loss 0.0059, train error 0.0018, valid error 0.0090\n",
      "epoch 3: loss 0.0048, train error 0.0015, valid error 0.0079\n",
      "epoch 4: loss 0.0040, train error 0.0013, valid error 0.0075\n",
      "epoch 5: loss 0.0035, train error 0.0011, valid error 0.0071\n",
      "epoch 6: loss 0.0030, train error 0.0010, valid error 0.0070\n",
      "epoch 7: loss 0.0026, train error 0.0009, valid error 0.0066\n",
      "epoch 8: loss 0.0022, train error 0.0007, valid error 0.0062\n",
      "epoch 9: loss 0.0019, train error 0.0006, valid error 0.0061\n",
      "epoch 10: loss 0.0016, train error 0.0005, valid error 0.0059\n",
      "USE #FILTER 128\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss nan, train error 0.0451, valid error 0.2252\n",
      "epoch 2: loss nan, train error 0.0451, valid error 0.2252\n",
      "epoch 3: loss nan, train error 0.0451, valid error 0.2252\n",
      "epoch 4: loss nan, train error 0.0451, valid error 0.2252\n",
      "epoch 5: loss nan, train error 0.0451, valid error 0.2252\n",
      "epoch 6: loss nan, train error 0.0451, valid error 0.2252\n",
      "epoch 7: loss nan, train error 0.0451, valid error 0.2252\n",
      "epoch 8: loss nan, train error 0.0451, valid error 0.2252\n",
      "epoch 9: loss nan, train error 0.0451, valid error 0.2252\n",
      "epoch 10: loss nan, train error 0.0451, valid error 0.2252\n",
      "USE #FILTER 256\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 2: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 3: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 4: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 5: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 6: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 7: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 8: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 9: loss nan, train error 0.0225, valid error 0.1126\n",
      "epoch 10: loss nan, train error 0.0225, valid error 0.1126\n",
      "USE CPU\n",
      "USE #FILTER 8\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0562, train error 0.0174, valid error 0.0286\n",
      "epoch 2: loss 0.0403, train error 0.0126, valid error 0.0226\n",
      "epoch 3: loss 0.0348, train error 0.0110, valid error 0.0215\n",
      "epoch 4: loss 0.0303, train error 0.0095, valid error 0.0189\n",
      "epoch 5: loss 0.0269, train error 0.0084, valid error 0.0190\n",
      "epoch 6: loss 0.0248, train error 0.0076, valid error 0.0184\n",
      "epoch 7: loss 0.0228, train error 0.0070, valid error 0.0175\n",
      "epoch 8: loss 0.0212, train error 0.0067, valid error 0.0171\n",
      "epoch 9: loss 0.0200, train error 0.0063, valid error 0.0170\n",
      "epoch 10: loss 0.0191, train error 0.0060, valid error 0.0165\n",
      "USE #FILTER 16\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0407, train error 0.0127, valid error 0.0341\n",
      "epoch 2: loss 0.0281, train error 0.0092, valid error 0.0246\n",
      "epoch 3: loss 0.0237, train error 0.0077, valid error 0.0223\n",
      "epoch 4: loss 0.0200, train error 0.0065, valid error 0.0198\n",
      "epoch 5: loss 0.0176, train error 0.0058, valid error 0.0183\n",
      "epoch 6: loss 0.0158, train error 0.0052, valid error 0.0168\n",
      "epoch 7: loss 0.0145, train error 0.0048, valid error 0.0159\n",
      "epoch 8: loss 0.0143, train error 0.0049, valid error 0.0163\n",
      "epoch 9: loss 0.0135, train error 0.0047, valid error 0.0164\n",
      "epoch 10: loss 0.0127, train error 0.0044, valid error 0.0165\n",
      "USE #FILTER 32\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0199, train error 0.0062, valid error 0.0268\n",
      "epoch 2: loss 0.0141, train error 0.0044, valid error 0.0222\n",
      "epoch 3: loss 0.0114, train error 0.0036, valid error 0.0202\n",
      "epoch 4: loss 0.0100, train error 0.0032, valid error 0.0188\n",
      "epoch 5: loss 0.0089, train error 0.0029, valid error 0.0181\n",
      "epoch 6: loss 0.0081, train error 0.0027, valid error 0.0177\n",
      "epoch 7: loss 0.0071, train error 0.0024, valid error 0.0168\n",
      "epoch 8: loss 0.0066, train error 0.0023, valid error 0.0160\n",
      "epoch 9: loss 0.0061, train error 0.0021, valid error 0.0157\n",
      "epoch 10: loss 0.0055, train error 0.0019, valid error 0.0154\n",
      "USE #FILTER 64\n",
      "... setup training\n",
      "... starting training\n",
      "epoch 1: loss 0.0091, train error 0.0028, valid error 0.0121\n",
      "epoch 2: loss 0.0059, train error 0.0019, valid error 0.0089\n",
      "epoch 3: loss 0.0050, train error 0.0016, valid error 0.0081\n",
      "epoch 4: loss 0.0042, train error 0.0014, valid error 0.0080\n",
      "epoch 5: loss 0.0038, train error 0.0012, valid error 0.0077\n",
      "epoch 6: loss 0.0034, train error 0.0012, valid error 0.0076\n",
      "epoch 7: loss 0.0029, train error 0.0010, valid error 0.0073\n",
      "epoch 8: loss 0.0026, train error 0.0009, valid error 0.0073\n",
      "epoch 9: loss 0.0023, train error 0.0008, valid error 0.0069\n",
      "epoch 10: loss 0.0021, train error 0.0007, valid error 0.0069\n"
     ]
    }
   ],
   "source": [
    "gpu_time = []\n",
    "cpu_time = []\n",
    "\n",
    "if len(gpus) > 0:\n",
    "    print(\"USE GPU\")\n",
    "    with tf.device(gpus[0].name):\n",
    "        gpu_time = check_time(gpu_filters)\n",
    "        \n",
    "if len(cpus) > 0:\n",
    "    print(\"USE CPU\")\n",
    "    with tf.device(cpus[0].name):\n",
    "        cpu_time = check_time(cpu_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAADtCAYAAACmli4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWaciqKsZJQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0f41a08470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mp\n",
    "if len(cpu_filters) == len(cpu_time):\n",
    "    plt.scatter(cpu_filters, cpu_time, label='CPU', c='C1', marker='s')\n",
    "\n",
    "if len(gpu_filters) == len(gpu_time):\n",
    "    plt.scatter(gpu_filters, gpu_time, label='GPU', c='C0', marker='o')\n",
    "    \n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('filter [\\#]')\n",
    "plt.ylim(-50, 950)\n",
    "plt.xscale('log', basex=2)\n",
    "\n",
    "plt.ylabel('runtime [s]')\n",
    "\n",
    "plt.savefig('../report/assets/runtime.pdf')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
